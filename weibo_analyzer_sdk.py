# -*- coding: utf-8 -*-
"""
å¾®åšçƒ­æœäº§å“åˆ›æ„åˆ†æå·¥å…· v2.0 (Claude Agent SDKå¢å¼ºç‰ˆ)
åŠŸèƒ½ï¼šè‡ªåŠ¨æŠ“å–å¾®åšçƒ­æœï¼Œè¿›è¡Œwebæœç´¢ï¼Œä½¿ç”¨Claude AIåˆ†æäº§å“åˆ›æ„ï¼Œç”ŸæˆHTMLæŠ¥å‘Š
"""

import sys
import io
import os
import json
import re
import time
import glob
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')

import requests
from bs4 import BeautifulSoup

# Claude Agent SDK (å¯é€‰)
try:
    import anthropic
    CLAUDE_AVAILABLE = True
except ImportError:
    CLAUDE_AVAILABLE = False
    print("âš ï¸  è­¦å‘Š: anthropicåŒ…æœªå®‰è£…ï¼Œå°†ä½¿ç”¨è§„åˆ™å¼•æ“æ¨¡å¼")
    print("   å®‰è£…å‘½ä»¤: pip install anthropic")

# ============================================================================
# é…ç½®åŠ è½½
# ============================================================================

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = Path(__file__).parent / "config.json"
    if config_path.exists():
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {
        "weibo_api": {
            "url": "https://apis.tianapi.com/weibohot/index",
            "key": os.environ.get("TIANAPI_KEY", "")
        },
        "analysis": {
            "default_count": 10,
            "enable_ai_analysis": True,
            "enable_web_search": True,
            "max_concurrent_searches": 5,
            "use_claude_sdk": CLAUDE_AVAILABLE
        },
        "output": {
            "directory": "output",
            "auto_open": False  # GitHub Actionsç¯å¢ƒç¦ç”¨è‡ªåŠ¨æ‰“å¼€
        }
    }

CONFIG = load_config()

# ============================================================================
# Claude Agent SDK äº§å“åˆ†æå™¨
# ============================================================================

class ClaudeProductAnalyzer:
    """ä½¿ç”¨Claude Agent SDKè¿›è¡Œäº§å“åˆ›æ„åˆ†æï¼ˆæ”¯æŒè‡ªå®šä¹‰APIç«¯ç‚¹ï¼‰"""

    def __init__(self):
        if not CLAUDE_AVAILABLE:
            raise ImportError("anthropicåŒ…æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install anthropic")

        api_key = os.environ.get("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEYç¯å¢ƒå˜é‡æœªè®¾ç½®")

        # æ”¯æŒè‡ªå®šä¹‰APIç«¯ç‚¹ï¼ˆå¦‚æ™ºè°±AIå…¼å®¹æ¥å£ï¼‰
        custom_api_url = os.environ.get("CUSTOM_API_URL")
        if custom_api_url:
            self.client = anthropic.Anthropic(api_key=api_key, base_url=custom_api_url)
            print(f"âœ“ ä½¿ç”¨è‡ªå®šä¹‰APIç«¯ç‚¹: {custom_api_url}")
        else:
            self.client = anthropic.Anthropic(api_key=api_key)

        # æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹ID
        self.model = os.environ.get("CUSTOM_MODEL_ID") or os.environ.get("CLAUDE_MODEL", "claude-3-5-sonnet-20241022")
        print(f"âœ“ AIåˆ†æå™¨å·²åˆå§‹åŒ– (æ¨¡å‹: {self.model})")

    def analyze_product_idea(self, topic: str, search_results: list) -> dict:
        """ä½¿ç”¨Claudeåˆ†æäº§å“åˆ›æ„"""

        # æ„å»ºä¸Šä¸‹æ–‡
        context = self._build_context(topic, search_results)

        # Claudeåˆ†ææç¤ºè¯
        prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„äº§å“åˆ›æ–°ä¸“å®¶å’Œå¸‚åœºåˆ†æå¸ˆã€‚è¯·åŸºäºä»¥ä¸‹çƒ­æœè¯é¢˜è¿›è¡Œæ·±åº¦äº§å“åˆ›æ„åˆ†æï¼š

{context}

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
{{
    "name": "äº§å“åç§°ï¼ˆä¸è¶…è¿‡15å­—ï¼Œè¦å¸å¼•çœ¼çƒï¼‰",
    "core_features": [
        "æ ¸å¿ƒåŠŸèƒ½1 - ç®€è¦æè¿°ï¼ˆä¸è¶…è¿‡30å­—ï¼‰",
        "æ ¸å¿ƒåŠŸèƒ½2 - ç®€è¦æè¿°ï¼ˆä¸è¶…è¿‡30å­—ï¼‰",
        "æ ¸å¿ƒåŠŸèƒ½3 - ç®€è¦æè¿°ï¼ˆä¸è¶…è¿‡30å­—ï¼‰",
        "æ ¸å¿ƒåŠŸèƒ½4 - ç®€è¦æè¿°ï¼ˆä¸è¶…è¿‡30å­—ï¼‰",
        "æ ¸å¿ƒåŠŸèƒ½5 - ç®€è¦æè¿°ï¼ˆä¸è¶…è¿‡30å­—ï¼‰"
    ],
    "market_pain_points": [
        "ç”¨æˆ·ç—›ç‚¹1 - å…·ä½“æè¿°ï¼ˆä¸è¶…è¿‡30å­—ï¼‰",
        "ç”¨æˆ·ç—›ç‚¹2 - å…·ä½“æè¿°ï¼ˆä¸è¶…è¿‡30å­—ï¼‰",
        "ç”¨æˆ·ç—›ç‚¹3 - å…·ä½“æè¿°ï¼ˆä¸è¶…è¿‡30å­—ï¼‰",
        "ç”¨æˆ·ç—›ç‚¹4 - å…·ä½“æè¿°ï¼ˆä¸è¶…è¿‡30å­—ï¼‰",
        "ç”¨æˆ·ç—›ç‚¹5 - å…·ä½“æè¿°ï¼ˆä¸è¶…è¿‡30å­—ï¼‰"
    ],
    "target_users": "ç›®æ ‡ç”¨æˆ·æè¿°ï¼ˆ50å­—å†…ï¼Œå…·ä½“ä¸”ç²¾å‡†ï¼‰",
    "innovation_points": [
        "åˆ›æ–°ç‚¹1ï¼ˆä¸è¶…è¿‡25å­—ï¼‰",
        "åˆ›æ–°ç‚¹2ï¼ˆä¸è¶…è¿‡25å­—ï¼‰",
        "åˆ›æ–°ç‚¹3ï¼ˆä¸è¶…è¿‡25å­—ï¼‰",
        "åˆ›æ–°ç‚¹4ï¼ˆä¸è¶…è¿‡25å­—ï¼‰",
        "åˆ›æ–°ç‚¹5ï¼ˆä¸è¶…è¿‡25å­—ï¼‰"
    ],
    "market_potential": {{
        "market_size": "å¸‚åœºè§„æ¨¡æè¿°ï¼ˆä¸è¶…è¿‡30å­—ï¼‰",
        "growth_stage": "å¢é•¿é˜¶æ®µï¼ˆå¦‚ï¼šå¿«é€Ÿæˆé•¿æœŸï¼‰",
        "competitive_advantage": "ç«äº‰ä¼˜åŠ¿æè¿°ï¼ˆä¸è¶…è¿‡40å­—ï¼‰",
        "revenue_model": "å•†ä¸šæ¨¡å¼æè¿°ï¼ˆä¸è¶…è¿‡30å­—ï¼‰"
    }},
    "scores": {{
        "innovation": åˆ›æ–°æ€§åˆ†æ•°(15-30ä¹‹é—´çš„æ•´æ•°),
        "pain_point": ç—›ç‚¹æ´å¯Ÿåˆ†æ•°(15-25ä¹‹é—´çš„æ•´æ•°),
        "potential": æ½œåŠ›ç©ºé—´åˆ†æ•°(10-15ä¹‹é—´çš„æ•´æ•°),
        "social": ç¤¾äº¤å±æ€§åˆ†æ•°(5-10ä¹‹é—´çš„æ•´æ•°),
        "practicality": å®ç”¨æ€§åˆ†æ•°(5-10ä¹‹é—´çš„æ•´æ•°),
        "feasibility": å¯è¡Œæ€§åˆ†æ•°(5-10ä¹‹é—´çš„æ•´æ•°)
    }}
}

è¯„åˆ†æ ‡å‡†ï¼š
- innovation (15-30åˆ†): æ¦‚å¿µæ–°é¢–ç¨‹åº¦ï¼Œé¢ è¦†æ€§åˆ›æ–°å¾—é«˜åˆ†
- pain_point (15-25åˆ†): æ˜¯å¦æŠ“ä½çœŸå®ç—›ç‚¹ï¼Œæ´å¯Ÿæ·±åˆ»å¾—é«˜åˆ†
- potential (10-15åˆ†): å¸‚åœºæ½œåŠ›å¤§å°ï¼Œç”¨æˆ·åŸºæ•°å’Œå¢é•¿ç©ºé—´
- social (5-10åˆ†): æ˜¯å¦å…·å¤‡ä¼ æ’­æ€§å’Œç¤¾äº¤è£‚å˜æ½œåŠ›
- practicality (5-10åˆ†): è§£å†³å®é™…é—®é¢˜çš„èƒ½åŠ›
- feasibility (5-10åˆ†): æŠ€æœ¯å®ç°éš¾åº¦ï¼Œå¯è¡Œæ€§é«˜å¾—é«˜åˆ†

è¯·ç¡®ä¿ï¼š
1. åˆ†æå…·ä½“ã€æœ‰æ´å¯ŸåŠ›ï¼Œé¿å…å¥—è¯å’Œæ³›æ³›è€Œè°ˆ
2. é’ˆå¯¹å…·ä½“çƒ­æœè¯é¢˜å®šåˆ¶åˆ†æï¼Œä¸è¦ç”¨æ¨¡æ¿åŒ–å†…å®¹
3. æå–è¯é¢˜ä¸­çš„å…³é”®ä¿¡æ¯ï¼ˆå¦‚å“ç‰Œåã€æ•°å­—ã€äº‹ä»¶ç­‰ï¼‰èå…¥åˆ†æ
4. ç›´æ¥è¿”å›JSONï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—æˆ–è§£é‡Š

ç°åœ¨è¯·å¼€å§‹åˆ†æï¼š"""

        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=2500,
                temperature=0.7,  # ç¨é«˜çš„æ¸©åº¦ä»¥è·å¾—æ›´æœ‰åˆ›æ„çš„è¾“å‡º
                messages=[{
                    "role": "user",
                    "content": prompt
                }]
            )

            # è§£æClaudeå“åº”
            result_text = response.content[0].text

            # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            if result_text.startswith("```json"):
                result_text = result_text[7:]
            if result_text.startswith("```"):
                result_text = result_text[3:]
            if result_text.endswith("```"):
                result_text = result_text[:-3]
            result_text = result_text.strip()

            analysis = json.loads(result_text)

            # è®¡ç®—æ€»åˆ†
            scores = analysis["scores"]
            interest_score = scores["innovation"] + scores["pain_point"] + scores["potential"] + scores["social"]
            utility_score = scores["practicality"] + scores["feasibility"]
            scores["total"] = round(interest_score + utility_score, 1)
            scores["interest_score"] = interest_score
            scores["utility_score"] = utility_score

            print(f"  âœ“ Claudeåˆ†æå®Œæˆ: {analysis['name']} (è¯„åˆ†: {scores['total']})")

            return analysis

        except json.JSONDecodeError as e:
            print(f"  âœ— Claudeå“åº”è§£æå¤±è´¥: {e}")
            print(f"  å“åº”å†…å®¹: {result_text[:200]}...")
            return self._fallback_analysis(topic, search_results)
        except Exception as e:
            print(f"  âœ— Claude APIè°ƒç”¨å¤±è´¥: {e}")
            return self._fallback_analysis(topic, search_results)

    def _build_context(self, topic: str, search_results: list) -> str:
        """æ„å»ºåˆ†æä¸Šä¸‹æ–‡"""
        context = f"""## çƒ­æœè¯é¢˜
{topic}

## è¯é¢˜åˆ†æ
è¯·åˆ†æè¿™ä¸ªçƒ­æœè¯é¢˜èƒŒååæ˜ çš„ç”¨æˆ·éœ€æ±‚ã€å¸‚åœºè¶‹åŠ¿å’Œç¤¾ä¼šæƒ…ç»ªã€‚

## èƒŒæ™¯ä¿¡æ¯
"""

        if search_results:
            context += "### ç›¸å…³æ–°é—»/è®¨è®º\n"
            for i, r in enumerate(search_results[:5], 1):
                context += f"{i}. {r['title']}\n"
        else:
            context += "ï¼ˆæš‚æ— æœç´¢ç»“æœï¼Œè¯·åŸºäºè¯é¢˜åç§°æœ¬èº«è¿›è¡Œåˆ†æï¼‰"

        return context

    def _fallback_analysis(self, topic: str, search_results: list) -> dict:
        """é™çº§åˆ°è§„åˆ™å¼•æ“åˆ†æ"""
        print(f"  âš ï¸ é™çº§åˆ°è§„åˆ™å¼•æ“æ¨¡å¼")
        # å¯¼å…¥åŸæœ‰çš„è§„åˆ™å¼•æ“
        from weibo_hot_analyzer import mock_ai_analysis
        return mock_ai_analysis(topic, search_results)


# ============================================================================
# å¾®åšçƒ­æœAPIè°ƒç”¨ï¼ˆå¤ç”¨åŸæœ‰ä»£ç ï¼‰
# ============================================================================

def fetch_weibo_hot(count=10):
    """è·å–å¾®åšçƒ­æœæ¦œå•ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
    url = CONFIG["weibo_api"]["url"]
    params = {
        "key": CONFIG["weibo_api"]["key"],
        "num": count
    }

    print(f"\n{'='*55}")
    print(f"   å¾®åšçƒ­æœäº§å“åˆ›æ„åˆ†æå·¥å…· v2.0 (Claude SDK)")
    print(f"{'='*55}")
    print(f"\næ­£åœ¨è·å–å¾®åšçƒ­æœTOP {count}...")

    # é‡è¯•æœºåˆ¶ï¼šæœ€å¤šå°è¯•3æ¬¡
    max_retries = 3
    for attempt in range(max_retries):
        try:
            print(f"  å°è¯•ç¬¬ {attempt + 1}/{max_retries} æ¬¡è¯·æ±‚...")
            response = requests.get(url, params=params, timeout=30)
            response.encoding = 'utf-8'
            data = response.json()

            if data.get("code") == 200:
                hot_list = data.get("result", {}).get("list", [])
                hot_list = hot_list[:count]
                print(f"âœ“ è·å–æˆåŠŸï¼å…± {len(hot_list)} æ¡çƒ­æœ\n")
                return hot_list
            else:
                print(f"  APIè¿”å›é”™è¯¯: {data.get('msg', 'æœªçŸ¥é”™è¯¯')}")
                if attempt < max_retries - 1:
                    print(f"  ç­‰å¾…2ç§’åé‡è¯•...")
                    time.sleep(2)
                else:
                    return get_backup_hot_list(count)
        except requests.exceptions.Timeout:
            print(f"  è¯·æ±‚è¶…æ—¶ï¼ˆ30ç§’ï¼‰")
            if attempt < max_retries - 1:
                print(f"  ç­‰å¾…2ç§’åé‡è¯•...")
                time.sleep(2)
            else:
                print(f"  æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ•°æ®")
                return get_backup_hot_list(count)
        except Exception as e:
            print(f"  è¯·æ±‚å¼‚å¸¸: {e}")
            if attempt < max_retries - 1:
                print(f"  ç­‰å¾…2ç§’åé‡è¯•...")
                time.sleep(2)
            else:
                print(f"  æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ•°æ®")
                return get_backup_hot_list(count)

    return get_backup_hot_list(count)

def get_backup_hot_list(count=10):
    """å¤‡ç”¨çƒ­æœåˆ—è¡¨ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    print("ä½¿ç”¨å¤‡ç”¨æ•°æ®...\n")
    return [
        {"hotWord": f"æµ‹è¯•çƒ­æœè¯é¢˜{i}", "hotRank": i, "hotScore": 1000000 - i * 10000}
        for i in range(1, count + 1)
    ]


# ============================================================================
# Webæœç´¢åŠŸèƒ½ï¼ˆå¤ç”¨åŸæœ‰ä»£ç ï¼‰
# ============================================================================

def web_search_topic(topic, max_results=3):
    """å¯¹çƒ­æœè¯é¢˜è¿›è¡Œwebæœç´¢ï¼ˆå¢å¼ºé”™è¯¯å¤„ç†ï¼‰"""
    if not CONFIG["analysis"].get("enable_web_search", True):
        return []

    try:
        search_query = f"{topic} æ–°é—» èƒŒæ™¯"
        encoded_query = requests.utils.quote(search_query)
        search_url = f"https://www.baidu.com/s?wd={encoded_query}"

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }

        time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
        response = requests.get(search_url, headers=headers, timeout=10)
        response.encoding = 'utf-8'

        soup = BeautifulSoup(response.text, 'html.parser')

        results = []
        # å°è¯•å¤šç§é€‰æ‹©å™¨
        selectors = ['.result', 'div[class*="result"]', '.c-container']

        for selector in selectors:
            items = soup.select(selector)[:max_results]
            if items:
                for item in items:
                    try:
                        title_elem = item.select_one('h3 a') or item.select_one('a')
                        if title_elem:
                            title = title_elem.get_text(strip=True)
                            href = title_elem.get('href', '')
                            if title and len(title) > 5:
                                results.append({
                                    "title": title[:100],  # é™åˆ¶é•¿åº¦
                                    "url": href
                                })
                                if len(results) >= max_results:
                                    break
                    except:
                        continue
                if results:
                    break

        return results

    except Exception as e:
        print(f"  [æœç´¢å¤±è´¥] {topic}: {str(e)[:50]}")
        return []


# ============================================================================
# äº§å“åˆ›æ„åˆ†æ
# ============================================================================

def analyze_product_idea(topic, search_results=[]):
    """åŸºäºçƒ­æœè¯é¢˜åˆ†æäº§å“åˆ›æ„"""

    # ä¼˜å…ˆä½¿ç”¨Claude SDK
    if CONFIG["analysis"].get("use_claude_sdk") and CLAUDE_AVAILABLE:
        try:
            analyzer = ClaudeProductAnalyzer()
            return analyzer.analyze_product_idea(topic, search_results)
        except Exception as e:
            print(f"  âš ï¸ Claude SDKä¸å¯ç”¨: {e}")
            print(f"  é™çº§åˆ°è§„åˆ™å¼•æ“...")

    # é™çº§åˆ°è§„åˆ™å¼•æ“
    from weibo_hot_analyzer import mock_ai_analysis
    return mock_ai_analysis(topic, search_results)


# ============================================================================
# äº‹ä»¶è„‰ç»œç”Ÿæˆï¼ˆå¤ç”¨åŸæœ‰ä»£ç ï¼‰
# ============================================================================

# å¯¼å…¥åŸæœ‰çš„HTMLç”Ÿæˆå‡½æ•°
def import_html_generator():
    """åŠ¨æ€å¯¼å…¥åŸæœ‰çš„HTMLç”Ÿæˆå‡½æ•°"""
    import importlib.util
    spec = importlib.util.spec_from_file_location("weibo_hot_analyzer",
                                                     Path(__file__).parent / "weibo_hot_analyzer.py")
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


# ============================================================================
# ä¸»æµç¨‹
# ============================================================================

def main(count=None):
    """ä¸»å‡½æ•°"""
    if count is None:
        count = CONFIG["analysis"]["default_count"]

    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨Claude SDK
    use_claude = CONFIG["analysis"].get("use_claude_sdk", False) and CLAUDE_AVAILABLE
    if use_claude:
        print(f"ğŸ¤– ä½¿ç”¨Claude Agent SDKè¿›è¡ŒAIåˆ†æ")
    else:
        print(f"ğŸ“‹ ä½¿ç”¨è§„åˆ™å¼•æ“æ¨¡å¼")

    # 1. è·å–å¾®åšçƒ­æœ
    hot_list = fetch_weibo_hot(count)
    if not hot_list:
        print("æœªèƒ½è·å–çƒ­æœæ•°æ®")
        return None

    # 2. åˆ†ææ¯ä¸ªçƒ­æœ
    print("æ­£åœ¨åˆ†æçƒ­æœè¯é¢˜...")
    hot_topics_with_analysis = []

    for i, hot in enumerate(hot_list, 1):
        topic = hot.get("hotword", hot.get("hotWord", hot.get("word", "")))
        rank = i

        print(f"  [{i}/{len(hot_list)}] åˆ†æ: {topic}")

        # Webæœç´¢
        search_results = []
        if CONFIG["analysis"]["enable_web_search"]:
            print(f"       - æ­£åœ¨æœç´¢ç›¸å…³ä¿¡æ¯...")
            search_results = web_search_topic(topic)

        # AIåˆ†æ
        print(f"       - æ­£åœ¨è¿›è¡Œäº§å“åˆ›æ„åˆ†æ...")
        analysis = analyze_product_idea(topic, search_results)

        hot_topics_with_analysis.append({
            "topic": topic,
            "rank": rank,
            "hot_score": hot.get("hotScore", 0),
            "search_results": search_results,
            "analysis": analysis
        })

        print()  # ç©ºè¡Œåˆ†éš”

    # 3. ç”ŸæˆHTMLæŠ¥å‘Šï¼ˆä½¿ç”¨åŸæœ‰çš„ç”Ÿæˆå‡½æ•°ï¼‰
    print("æ­£åœ¨ç”ŸæˆHTMLæŠ¥å‘Š...")
    module = import_html_generator()
    html_content, filename = module.generate_html_report(hot_topics_with_analysis)

    output_dir = Path(__file__).parent / CONFIG["output"]["directory"]
    output_dir.mkdir(exist_ok=True)
    output_path = output_dir / filename

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(html_content)

    # 4. è¾“å‡ºç»“æœ
    print(f"\n{'='*55}")
    print("   åˆ†æå®Œæˆï¼")
    print(f"{'='*55}")
    print(f"\næŠ¥å‘Šå·²ä¿å­˜: {output_path.absolute()}")
    print(f"\nğŸ“Š åˆ†ææ¦‚å†µ:")

    sorted_by_score = sorted(
        hot_topics_with_analysis,
        key=lambda x: x['analysis']['scores']['total'],
        reverse=True
    )

    excellent = sum(1 for t in sorted_by_score if t['analysis']['scores']['total'] >= 80)
    good = sum(1 for t in sorted_by_score if 60 <= t['analysis']['scores']['total'] < 80)

    print(f"  - åˆ†æçƒ­ç‚¹: {len(hot_topics_with_analysis)}ä¸ª")
    print(f"  - ä¼˜ç§€åˆ›æ„(â‰¥80åˆ†): {excellent}ä¸ª")
    print(f"  - è‰¯å¥½åˆ›æ„(60-80åˆ†): {good}ä¸ª")

    print(f"\nğŸŒŸ TOP3 ä¼˜ç§€åˆ›æ„:")
    for i, t in enumerate(sorted_by_score[:3], 1):
        print(f"  {i}. {t['analysis']['name']}")
        print(f"     è¯„åˆ†: {t['analysis']['scores']['total']}åˆ†")
        print(f"     æ ¸å¿ƒ: {', '.join(t['analysis']['core_features'][:2])}")

    # è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼ˆæœ¬åœ°è¿è¡Œæ—¶ï¼‰
    if CONFIG["output"].get("auto_open", False):
        try:
            import webbrowser
            webbrowser.open(f'file://{output_path.absolute()}')
            print(f"\nå·²åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æŠ¥å‘Š")
        except:
            pass

    return str(output_path.absolute())


if __name__ == "__main__":
    count = int(sys.argv[1]) if len(sys.argv) > 1 else None
    main(count)
