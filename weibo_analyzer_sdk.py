# -*- coding: utf-8 -*-
"""
å¾®åšçƒ­æœäº§å“åˆ›æ„åˆ†æžå·¥å…· v2.0 (Claude Agent SDKå¢žå¼ºç‰ˆ)
åŠŸèƒ½ï¼šè‡ªåŠ¨æŠ“å–å¾®åšçƒ­æœï¼Œè¿›è¡Œwebæœç´¢ï¼Œä½¿ç”¨Claude AIåˆ†æžäº§å“åˆ›æ„ï¼Œç”ŸæˆHTMLæŠ¥å‘Š
"""

import sys
import io
import os
import json
import re
import time
import glob
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')

import requests
from bs4 import BeautifulSoup

# Claude Agent SDK (å¯é€‰)
try:
    import anthropic
    CLAUDE_AVAILABLE = True
except ImportError:
    CLAUDE_AVAILABLE = False
    print("âš ï¸  è­¦å‘Š: anthropicåŒ…æœªå®‰è£…ï¼Œå°†ä½¿ç”¨è§„åˆ™å¼•æ“Žæ¨¡å¼")
    print("   å®‰è£…å‘½ä»¤: pip install anthropic")

# ============================================================================
# é…ç½®åŠ è½½
# ============================================================================

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = Path(__file__).parent / "config.json"
    if config_path.exists():
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {
        "weibo_api": {
            "url": "https://apis.tianapi.com/weibohot/index",
            "key": os.environ.get("TIANAPI_KEY", "")
        },
        "analysis": {
            "default_count": 10,
            "enable_ai_analysis": True,
            "enable_web_search": True,
            "max_concurrent_searches": 5,
            "use_claude_sdk": CLAUDE_AVAILABLE
        },
        "output": {
            "directory": "output",
            "auto_open": False  # GitHub ActionsçŽ¯å¢ƒç¦ç”¨è‡ªåŠ¨æ‰“å¼€
        }
    }

CONFIG = load_config()

# ============================================================================
# Claude Agent SDK äº§å“åˆ†æžå™¨
# ============================================================================

class ClaudeProductAnalyzer:
    """ä½¿ç”¨Claude Agent SDKè¿›è¡Œäº§å“åˆ›æ„åˆ†æžï¼ˆæ”¯æŒè‡ªå®šä¹‰APIç«¯ç‚¹ï¼‰"""

    def __init__(self):
        if not CLAUDE_AVAILABLE:
            raise ImportError("anthropicåŒ…æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install anthropic")

        api_key = os.environ.get("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEYçŽ¯å¢ƒå˜é‡æœªè®¾ç½®")

        # æ”¯æŒè‡ªå®šä¹‰APIç«¯ç‚¹ï¼ˆå¦‚æ™ºè°±AIå…¼å®¹æŽ¥å£ï¼‰
        custom_api_url = os.environ.get("CUSTOM_API_URL")
        if custom_api_url:
            self.client = anthropic.Anthropic(api_key=api_key, base_url=custom_api_url)
            print(f"âœ“ ä½¿ç”¨è‡ªå®šä¹‰APIç«¯ç‚¹: {custom_api_url}")
        else:
            self.client = anthropic.Anthropic(api_key=api_key)

        # æ”¯æŒè‡ªå®šä¹‰æ¨¡åž‹ID
        self.model = os.environ.get("CUSTOM_MODEL_ID") or os.environ.get("CLAUDE_MODEL", "claude-3-5-sonnet-20241022")
        print(f"âœ“ AIåˆ†æžå™¨å·²åˆå§‹åŒ– (æ¨¡åž‹: {self.model})")

    def analyze_product_idea(self, topic: str, search_results: list) -> dict:
        """ä½¿ç”¨Claudeåˆ†æžäº§å“åˆ›æ„"""

        # æž„å»ºä¸Šä¸‹æ–‡
        context = self._build_context(topic, search_results)

        # Claudeåˆ†æžæç¤ºè¯
        prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„äº§å“åˆ›æ–°ä¸“å®¶å’Œå¸‚åœºåˆ†æžå¸ˆã€‚è¯·åŸºäºŽä»¥ä¸‹çƒ­æœè¯é¢˜è¿›è¡Œæ·±åº¦äº§å“åˆ›æ„åˆ†æžï¼š

{context}

è¯·ä»¥JSONæ ¼å¼è¿”å›žåˆ†æžç»“æžœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
{{
    "name": "äº§å“åç§°ï¼ˆä¸è¶…è¿‡15å­—ï¼Œè¦å¸å¼•çœ¼çƒï¼‰",
    "core_features": [
        "æ ¸å¿ƒåŠŸèƒ½1 - ç®€è¦æè¿°ï¼ˆä¸è¶…è¿‡30å­—ï¼‰",
        "æ ¸å¿ƒåŠŸèƒ½2 - ç®€è¦æè¿°ï¼ˆä¸è¶…è¿‡30å­—ï¼‰",
        "æ ¸å¿ƒåŠŸèƒ½3 - ç®€è¦æè¿°ï¼ˆä¸è¶…è¿‡30å­—ï¼‰",
        "æ ¸å¿ƒåŠŸèƒ½4 - ç®€è¦æè¿°ï¼ˆä¸è¶…è¿‡30å­—ï¼‰",
        "æ ¸å¿ƒåŠŸèƒ½5 - ç®€è¦æè¿°ï¼ˆä¸è¶…è¿‡30å­—ï¼‰"
    ],
    "market_pain_points": [
        "ç”¨æˆ·ç—›ç‚¹1 - å…·ä½“æè¿°ï¼ˆä¸è¶…è¿‡30å­—ï¼‰",
        "ç”¨æˆ·ç—›ç‚¹2 - å…·ä½“æè¿°ï¼ˆä¸è¶…è¿‡30å­—ï¼‰",
        "ç”¨æˆ·ç—›ç‚¹3 - å…·ä½“æè¿°ï¼ˆä¸è¶…è¿‡30å­—ï¼‰",
        "ç”¨æˆ·ç—›ç‚¹4 - å…·ä½“æè¿°ï¼ˆä¸è¶…è¿‡30å­—ï¼‰",
        "ç”¨æˆ·ç—›ç‚¹5 - å…·ä½“æè¿°ï¼ˆä¸è¶…è¿‡30å­—ï¼‰"
    ],
    "target_users": "ç›®æ ‡ç”¨æˆ·æè¿°ï¼ˆ50å­—å†…ï¼Œå…·ä½“ä¸”ç²¾å‡†ï¼‰",
    "innovation_points": [
        "åˆ›æ–°ç‚¹1ï¼ˆä¸è¶…è¿‡25å­—ï¼‰",
        "åˆ›æ–°ç‚¹2ï¼ˆä¸è¶…è¿‡25å­—ï¼‰",
        "åˆ›æ–°ç‚¹3ï¼ˆä¸è¶…è¿‡25å­—ï¼‰",
        "åˆ›æ–°ç‚¹4ï¼ˆä¸è¶…è¿‡25å­—ï¼‰",
        "åˆ›æ–°ç‚¹5ï¼ˆä¸è¶…è¿‡25å­—ï¼‰"
    ],
    "market_potential": {{
        "market_size": "å¸‚åœºè§„æ¨¡æè¿°ï¼ˆä¸è¶…è¿‡30å­—ï¼‰",
        "growth_stage": "å¢žé•¿é˜¶æ®µï¼ˆå¦‚ï¼šå¿«é€Ÿæˆé•¿æœŸï¼‰",
        "competitive_advantage": "ç«žäº‰ä¼˜åŠ¿æè¿°ï¼ˆä¸è¶…è¿‡40å­—ï¼‰",
        "revenue_model": "å•†ä¸šæ¨¡å¼æè¿°ï¼ˆä¸è¶…è¿‡30å­—ï¼‰"
    }},
    "scores": {{
        "innovation": åˆ›æ–°æ€§åˆ†æ•°(15-30ä¹‹é—´çš„æ•´æ•°),
        "pain_point": ç—›ç‚¹æ´žå¯Ÿåˆ†æ•°(15-25ä¹‹é—´çš„æ•´æ•°),
        "potential": æ½œåŠ›ç©ºé—´åˆ†æ•°(10-15ä¹‹é—´çš„æ•´æ•°),
        "social": ç¤¾äº¤å±žæ€§åˆ†æ•°(5-10ä¹‹é—´çš„æ•´æ•°),
        "practicality": å®žç”¨æ€§åˆ†æ•°(5-10ä¹‹é—´çš„æ•´æ•°),
        "feasibility": å¯è¡Œæ€§åˆ†æ•°(5-10ä¹‹é—´çš„æ•´æ•°)
    }}
}

è¯„åˆ†æ ‡å‡†ï¼š
- innovation (15-30åˆ†): æ¦‚å¿µæ–°é¢–ç¨‹åº¦ï¼Œé¢ è¦†æ€§åˆ›æ–°å¾—é«˜åˆ†
- pain_point (15-25åˆ†): æ˜¯å¦æŠ“ä½çœŸå®žç—›ç‚¹ï¼Œæ´žå¯Ÿæ·±åˆ»å¾—é«˜åˆ†
- potential (10-15åˆ†): å¸‚åœºæ½œåŠ›å¤§å°ï¼Œç”¨æˆ·åŸºæ•°å’Œå¢žé•¿ç©ºé—´
- social (5-10åˆ†): æ˜¯å¦å…·å¤‡ä¼ æ’­æ€§å’Œç¤¾äº¤è£‚å˜æ½œåŠ›
- practicality (5-10åˆ†): è§£å†³å®žé™…é—®é¢˜çš„èƒ½åŠ›
- feasibility (5-10åˆ†): æŠ€æœ¯å®žçŽ°éš¾åº¦ï¼Œå¯è¡Œæ€§é«˜å¾—é«˜åˆ†

è¯·ç¡®ä¿ï¼š
1. åˆ†æžå…·ä½“ã€æœ‰æ´žå¯ŸåŠ›ï¼Œé¿å…å¥—è¯å’Œæ³›æ³›è€Œè°ˆ
2. é’ˆå¯¹å…·ä½“çƒ­æœè¯é¢˜å®šåˆ¶åˆ†æžï¼Œä¸è¦ç”¨æ¨¡æ¿åŒ–å†…å®¹
3. æå–è¯é¢˜ä¸­çš„å…³é”®ä¿¡æ¯ï¼ˆå¦‚å“ç‰Œåã€æ•°å­—ã€äº‹ä»¶ç­‰ï¼‰èžå…¥åˆ†æž
4. ç›´æŽ¥è¿”å›žJSONï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—æˆ–è§£é‡Š

çŽ°åœ¨è¯·å¼€å§‹åˆ†æžï¼š"""

        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=2500,
                temperature=0.7,  # ç¨é«˜çš„æ¸©åº¦ä»¥èŽ·å¾—æ›´æœ‰åˆ›æ„çš„è¾“å‡º
                messages=[{
                    "role": "user",
                    "content": prompt
                }]
            )

            # è§£æžClaudeå“åº”
            result_text = response.content[0].text

            # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            if result_text.startswith("```json"):
                result_text = result_text[7:]
            if result_text.startswith("```"):
                result_text = result_text[3:]
            if result_text.endswith("```"):
                result_text = result_text[:-3]
            result_text = result_text.strip()

            analysis = json.loads(result_text)

            # è®¡ç®—æ€»åˆ†
            scores = analysis["scores"]
            interest_score = scores["innovation"] + scores["pain_point"] + scores["potential"] + scores["social"]
            utility_score = scores["practicality"] + scores["feasibility"]
            scores["total"] = round(interest_score + utility_score, 1)
            scores["interest_score"] = interest_score
            scores["utility_score"] = utility_score

            print(f"  âœ“ Claudeåˆ†æžå®Œæˆ: {analysis['name']} (è¯„åˆ†: {scores['total']})")

            return analysis

        except json.JSONDecodeError as e:
            print(f"  âœ— Claudeå“åº”è§£æžå¤±è´¥: {e}")
            print(f"  å“åº”å†…å®¹: {result_text[:200]}...")
            return self._fallback_analysis(topic, search_results)
        except Exception as e:
            print(f"  âœ— Claude APIè°ƒç”¨å¤±è´¥: {e}")
            return self._fallback_analysis(topic, search_results)

    def _build_context(self, topic: str, search_results: list) -> str:
        """æž„å»ºåˆ†æžä¸Šä¸‹æ–‡"""
        context = f"""## çƒ­æœè¯é¢˜
{topic}

## è¯é¢˜åˆ†æž
è¯·åˆ†æžè¿™ä¸ªçƒ­æœè¯é¢˜èƒŒåŽåæ˜ çš„ç”¨æˆ·éœ€æ±‚ã€å¸‚åœºè¶‹åŠ¿å’Œç¤¾ä¼šæƒ…ç»ªã€‚

## èƒŒæ™¯ä¿¡æ¯
"""

        if search_results:
            context += "### ç›¸å…³æ–°é—»/è®¨è®º\n"
            for i, r in enumerate(search_results[:5], 1):
                context += f"{i}. {r['title']}\n"
        else:
            context += "ï¼ˆæš‚æ— æœç´¢ç»“æžœï¼Œè¯·åŸºäºŽè¯é¢˜åç§°æœ¬èº«è¿›è¡Œåˆ†æžï¼‰"

        return context

    def _fallback_analysis(self, topic: str, search_results: list) -> dict:
        """é™çº§åˆ°è§„åˆ™å¼•æ“Žåˆ†æž"""
        print(f"  âš ï¸ é™çº§åˆ°è§„åˆ™å¼•æ“Žæ¨¡å¼")
        # å¯¼å…¥åŽŸæœ‰çš„è§„åˆ™å¼•æ“Ž
        from weibo_hot_analyzer import mock_ai_analysis
        return mock_ai_analysis(topic, search_results)


# ============================================================================
# å¾®åšçƒ­æœAPIè°ƒç”¨ï¼ˆå¤ç”¨åŽŸæœ‰ä»£ç ï¼‰
# ============================================================================

def fetch_weibo_hot(count=10):
    """èŽ·å–å¾®åšçƒ­æœæ¦œå•"""
    url = CONFIG["weibo_api"]["url"]
    params = {
        "key": CONFIG["weibo_api"]["key"],
        "num": count
    }

    print(f"\n{'='*55}")
    print(f"   å¾®åšçƒ­æœäº§å“åˆ›æ„åˆ†æžå·¥å…· v2.0 (Claude SDK)")
    print(f"{'='*55}")
    print(f"\næ­£åœ¨èŽ·å–å¾®åšçƒ­æœTOP {count}...")

    try:
        response = requests.get(url, params=params, timeout=15)
        response.encoding = 'utf-8'
        data = response.json()

        if data.get("code") == 200:
            hot_list = data.get("result", {}).get("list", [])
            hot_list = hot_list[:count]
            print(f"èŽ·å–æˆåŠŸï¼å…± {len(hot_list)} æ¡çƒ­æœ\n")
            return hot_list
        else:
            print(f"APIè¿”å›žé”™è¯¯: {data.get('msg', 'æœªçŸ¥é”™è¯¯')}")
            return get_backup_hot_list(count)
    except Exception as e:
        print(f"è¯·æ±‚å¤±è´¥: {e}")
        return get_backup_hot_list(count)

def get_backup_hot_list(count=10):
    """å¤‡ç”¨çƒ­æœåˆ—è¡¨ï¼ˆç”¨äºŽæµ‹è¯•ï¼‰"""
    print("ä½¿ç”¨å¤‡ç”¨æ•°æ®...\n")
    return [
        {"hotWord": f"æµ‹è¯•çƒ­æœè¯é¢˜{i}", "hotRank": i, "hotScore": 1000000 - i * 10000}
        for i in range(1, count + 1)
    ]


# ============================================================================
# Webæœç´¢åŠŸèƒ½ï¼ˆå¤ç”¨åŽŸæœ‰ä»£ç ï¼‰
# ============================================================================

def web_search_topic(topic, max_results=3):
    """å¯¹çƒ­æœè¯é¢˜è¿›è¡Œwebæœç´¢"""
    if not CONFIG["analysis"]["enable_web_search"]:
        return []

    search_query = f"{topic} æ–°é—» èƒŒæ™¯"
    encoded_query = requests.utils.quote(search_query)
    search_url = f"https://www.baidu.com/s?wd={encoded_query}"

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }

    try:
        time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
        response = requests.get(search_url, headers=headers, timeout=10)
        response.encoding = 'utf-8'

        soup = BeautifulSoup(response.text, 'html.parser')

        results = []
        for item in soup.select('.result')[:max_results]:
            title_elem = item.select_one('h3 a')
            if title_elem:
                title = title_elem.get_text(strip=True)
                results.append({
                    "title": title,
                    "url": title_elem.get('href', '')
                })

        return results
    except Exception as e:
        print(f"  [æœç´¢å¤±è´¥] {topic}: {e}")
        return []


# ============================================================================
# äº§å“åˆ›æ„åˆ†æž
# ============================================================================

def analyze_product_idea(topic, search_results=[]):
    """åŸºäºŽçƒ­æœè¯é¢˜åˆ†æžäº§å“åˆ›æ„"""

    # ä¼˜å…ˆä½¿ç”¨Claude SDK
    if CONFIG["analysis"].get("use_claude_sdk") and CLAUDE_AVAILABLE:
        try:
            analyzer = ClaudeProductAnalyzer()
            return analyzer.analyze_product_idea(topic, search_results)
        except Exception as e:
            print(f"  âš ï¸ Claude SDKä¸å¯ç”¨: {e}")
            print(f"  é™çº§åˆ°è§„åˆ™å¼•æ“Ž...")

    # é™çº§åˆ°è§„åˆ™å¼•æ“Ž
    from weibo_hot_analyzer import mock_ai_analysis
    return mock_ai_analysis(topic, search_results)


# ============================================================================
# äº‹ä»¶è„‰ç»œç”Ÿæˆï¼ˆå¤ç”¨åŽŸæœ‰ä»£ç ï¼‰
# ============================================================================

# å¯¼å…¥åŽŸæœ‰çš„HTMLç”Ÿæˆå‡½æ•°
def import_html_generator():
    """åŠ¨æ€å¯¼å…¥åŽŸæœ‰çš„HTMLç”Ÿæˆå‡½æ•°"""
    import importlib.util
    spec = importlib.util.spec_from_file_location("weibo_hot_analyzer",
                                                     Path(__file__).parent / "weibo_hot_analyzer.py")
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


# ============================================================================
# ä¸»æµç¨‹
# ============================================================================

def main(count=None):
    """ä¸»å‡½æ•°"""
    if count is None:
        count = CONFIG["analysis"]["default_count"]

    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨Claude SDK
    use_claude = CONFIG["analysis"].get("use_claude_sdk", False) and CLAUDE_AVAILABLE
    if use_claude:
        print(f"ðŸ¤– ä½¿ç”¨Claude Agent SDKè¿›è¡ŒAIåˆ†æž")
    else:
        print(f"ðŸ“‹ ä½¿ç”¨è§„åˆ™å¼•æ“Žæ¨¡å¼")

    # 1. èŽ·å–å¾®åšçƒ­æœ
    hot_list = fetch_weibo_hot(count)
    if not hot_list:
        print("æœªèƒ½èŽ·å–çƒ­æœæ•°æ®")
        return None

    # 2. åˆ†æžæ¯ä¸ªçƒ­æœ
    print("æ­£åœ¨åˆ†æžçƒ­æœè¯é¢˜...")
    hot_topics_with_analysis = []

    for i, hot in enumerate(hot_list, 1):
        topic = hot.get("hotword", hot.get("hotWord", hot.get("word", "")))
        rank = i

        print(f"  [{i}/{len(hot_list)}] åˆ†æž: {topic}")

        # Webæœç´¢
        search_results = []
        if CONFIG["analysis"]["enable_web_search"]:
            print(f"       - æ­£åœ¨æœç´¢ç›¸å…³ä¿¡æ¯...")
            search_results = web_search_topic(topic)

        # AIåˆ†æž
        print(f"       - æ­£åœ¨è¿›è¡Œäº§å“åˆ›æ„åˆ†æž...")
        analysis = analyze_product_idea(topic, search_results)

        hot_topics_with_analysis.append({
            "topic": topic,
            "rank": rank,
            "hot_score": hot.get("hotScore", 0),
            "search_results": search_results,
            "analysis": analysis
        })

        print()  # ç©ºè¡Œåˆ†éš”

    # 3. ç”ŸæˆHTMLæŠ¥å‘Šï¼ˆä½¿ç”¨åŽŸæœ‰çš„ç”Ÿæˆå‡½æ•°ï¼‰
    print("æ­£åœ¨ç”ŸæˆHTMLæŠ¥å‘Š...")
    module = import_html_generator()
    html_content, filename = module.generate_html_report(hot_topics_with_analysis)

    output_dir = Path(__file__).parent / CONFIG["output"]["directory"]
    output_dir.mkdir(exist_ok=True)
    output_path = output_dir / filename

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(html_content)

    # 4. è¾“å‡ºç»“æžœ
    print(f"\n{'='*55}")
    print("   åˆ†æžå®Œæˆï¼")
    print(f"{'='*55}")
    print(f"\næŠ¥å‘Šå·²ä¿å­˜: {output_path.absolute()}")
    print(f"\nðŸ“Š åˆ†æžæ¦‚å†µ:")

    sorted_by_score = sorted(
        hot_topics_with_analysis,
        key=lambda x: x['analysis']['scores']['total'],
        reverse=True
    )

    excellent = sum(1 for t in sorted_by_score if t['analysis']['scores']['total'] >= 80)
    good = sum(1 for t in sorted_by_score if 60 <= t['analysis']['scores']['total'] < 80)

    print(f"  - åˆ†æžçƒ­ç‚¹: {len(hot_topics_with_analysis)}ä¸ª")
    print(f"  - ä¼˜ç§€åˆ›æ„(â‰¥80åˆ†): {excellent}ä¸ª")
    print(f"  - è‰¯å¥½åˆ›æ„(60-80åˆ†): {good}ä¸ª")

    print(f"\nðŸŒŸ TOP3 ä¼˜ç§€åˆ›æ„:")
    for i, t in enumerate(sorted_by_score[:3], 1):
        print(f"  {i}. {t['analysis']['name']}")
        print(f"     è¯„åˆ†: {t['analysis']['scores']['total']}åˆ†")
        print(f"     æ ¸å¿ƒ: {', '.join(t['analysis']['core_features'][:2])}")

    # è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼ˆæœ¬åœ°è¿è¡Œæ—¶ï¼‰
    if CONFIG["output"].get("auto_open", False):
        try:
            import webbrowser
            webbrowser.open(f'file://{output_path.absolute()}')
            print(f"\nå·²åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æŠ¥å‘Š")
        except:
            pass

    return str(output_path.absolute())


if __name__ == "__main__":
    count = int(sys.argv[1]) if len(sys.argv) > 1 else None
    main(count)
